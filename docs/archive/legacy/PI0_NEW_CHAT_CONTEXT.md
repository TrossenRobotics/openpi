# Pi0 - Kontext f√ºr neuen Chat

**Erstellt:** 07.01.2025, 14:18 Uhr  
**Zweck:** Vollst√§ndige Zusammenfassung f√ºr Fortsetzung in neuem Chat  
**Phase:** Training abgeschlossen ‚Üí Inference Setup

---

## üéØ AKTUELLE SITUATION

### Training Phase: ‚úÖ ERFOLGREICH ABGESCHLOSSEN

**Training l√§uft gerade auf:**
- **Maschine:** Training Server (SSH-Verbindung)
- **GPU:** NVIDIA RTX 6000 Pro 96GB (Blackwell)
- **Location:** `~/openpi/`
- **Status:** Training aktiv, WandB logging l√§uft

**Training Details:**
```
Config: pi0_lighter_cup_trossen
Dataset: MaxFridge/lighter_cup_v2 (92 Episodes, 4 Cameras)
Steps: 20.000 (batch size 32)
Erwartete Dauer: ~3-4 Stunden
Checkpoints: ~/openpi/checkpoints/pi0_lighter_cup_trossen/production_v1/
WandB: https://wandb.ai/sourteig-fritsch-gmbh/openpi
```

---

## üìä WAS BEREITS FUNKTIONIERT

### Training Server Setup (RTX 6000 Pro 96GB)

**Installation:**
- ‚úÖ openpi Repository: `~/openpi/`
- ‚úÖ UV Environment: 242 Packages, Python 3.11
- ‚úÖ JAX 0.5.3 + GPU Support verifiziert
- ‚úÖ Conda Environments (lerobot, trossenai) unber√ºhrt

**Konfiguration:**
- ‚úÖ Training Config erstellt & funktionsf√§hig
- ‚úÖ 2 Configs: test (1k steps) + production (20k steps)
- ‚úÖ Dataset: MaxFridge/lighter_cup_v2
- ‚úÖ Camera Mapping: 4 Cameras (cam_high, cam_low, left_wrist, right_wrist)
- ‚úÖ Normalization Stats berechnet
- ‚úÖ Alle Config-Probleme gel√∂st (Circular Import, repo_id, AssetsConfig)

**Dokumentation:**
- ‚úÖ 9 umfassende Guides in `~/lerobot/docs/PI0_*.md`

**Wichtige gel√∂ste Probleme:**
1. Circular Import bei Config-Loading ‚Üí Behoben
2. repo_id falsch (lighter_cup_v2episodes vs. MaxFridge/lighter_cup_v2) ‚Üí Korrigiert
3. AssetsConfig suchte auf Google Cloud statt lokal ‚Üí Angepasst
4. UV vs. Conda Verst√§ndnis ‚Üí Gekl√§rt (UV nutzt KEIN Conda!)

---

## üéØ WAS NOCH ZU TUN IST

### N√§chste Phase: Inference Setup

**Zwei Maschinen:**

1. **Training Server** (aktuell, SSH):
   - Policy Server einrichten
   - Netzwerk konfigurieren
   - Server im Hintergrund laufen lassen

2. **Inference PC** (RTX 4080 16GB, lokal mit Hardware):
   - openpi Client Environment installieren
   - LeRobot V0.3.2 mit BiWidowXAIFollower
   - Hardware-Integration
   - Client Script erstellen

**Gesch√§tzte Zeit:** 1-2 Tage

---

## üèóÔ∏è ARCHITEKTUR

### Server-Client System

```
[Training Server: RTX 6000 Pro]
         ‚Üì
    Policy Server (Port 8000)
    - L√§dt trained checkpoint
    - JAX Inference
    - WebSocket Listener
         ‚Üì
    Netzwerk (Gigabit LAN)
         ‚Üì
[Inference PC: RTX 4080]
         ‚Üì
    Robot Client
    - LeRobot V0.3.2
    - BiWidowXAIFollower
    - WebSocket Client
    - Control Loop (50Hz)
         ‚Üì
    Trossen AI Hardware
    - 2x WidowX Arms (14 DOF)
    - 4x Cameras (480x640)
```

---

## ‚ùì ROS2 & DOCKER - NICHT N√ñTIG!

**Klarstellung:**
- **ROS2:** Nur f√ºr andere Trossen Tutorials (MoveIt, etc.) - Pi0 nutzt KEIN ROS2
- **Docker:** Optional, nicht zwingend - Native UV Installation ist einfacher
- **Was wir nutzen:** Direkte Python API √ºber LeRobot + openpi

---

## üìã IMPLEMENTIERUNGSPLAN

### Auf Training Server (wo du jetzt per SSH bist):

#### Schritt 1: Policy Server vorbereiten

```bash
# Nach Training fertig (warte auf 20k steps):
cd ~/openpi

# Checkpoint ausw√§hlen
ls -lh checkpoints/pi0_lighter_cup_trossen/production_v1/

# Policy Server starten
uv run scripts/serve_policy.py policy:checkpoint \
  --policy.config=pi0_lighter_cup_trossen \
  --policy.dir=checkpoints/pi0_lighter_cup_trossen/production_v1/20000 \
  --host=0.0.0.0 \
  --port=8000
```

#### Schritt 2: Netzwerk konfigurieren

```bash
# Firewall √∂ffnen
sudo ufw allow 8000/tcp

# IP notieren
hostname -I
# z.B. 192.168.1.100 ‚Üí brauchst du f√ºr Client!

# Health check
curl http://localhost:8000/health
```

#### Schritt 3: Server im Hintergrund

```bash
# Mit tmux (empfohlen)
tmux new -s policy_server
# Server Command von oben
# Detach: Ctrl+B dann D
```

### Auf Inference PC (RTX 4080, lokal mit Hardware):

#### Schritt 1: Client Environment installieren

```bash
cd ~/
git clone --recurse-submodules https://github.com/TrossenRobotics/openpi.git openpi_client

cd openpi_client/examples/trossen_ai
GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .

# Verify LeRobot V0.3.2
uv run python -c "import lerobot; print(lerobot.__version__)"
```

#### Schritt 2: Hardware Config √ºbernehmen

```bash
# Nutze deine bestehende LeRobot Hardware Config!
cd ~/lerobot
# Finde Motor/Camera Configs
# √úbernehme f√ºr Client Script
```

#### Schritt 3: Client Script erstellen

```bash
# Template in docs/PI0_INFERENCE_COMPLETE_GUIDE.md
# Erstelle ~/openpi_client/examples/trossen_ai/run_inference_client.py
```

#### Schritt 4: Verbindung testen

```bash
# Netzwerk testen
ping <training_server_ip>

# WebSocket testen
python3 -c "import asyncio, websockets; ..."

# Client starten
uv run python run_inference_client.py --policy_url ws://<server_ip>:8000
```

---

## üìö DOKUMENTATION (9 Guides)

**Alle in `~/lerobot/docs/`:**

1. **PI0_NEW_CHAT_CONTEXT.md** ‚≠ê DIESES DOKUMENT
   - Komplette Zusammenfassung
   - Aktueller Stand
   - N√§chste Schritte

2. **PI0_INFERENCE_COMPLETE_GUIDE.md** ‚≠ê HAUPTGUIDE
   - Policy Server Setup (Training Server)
   - Robot Client Setup (Inference PC)
   - Netzwerk-Konfiguration
   - Hardware-Integration
   - Testing Workflow

3. **PI0_MIGRATION_PLAN.md**
   - Urspr√ºnglicher Gesamtplan
   - 6 Phasen (1-4 abgeschlossen)

4. **PI0_ENVIRONMENT_GUIDE.md**
   - UV vs. Conda Workflow
   - Wichtig: UV nutzt KEIN Conda!

5. **PI0_QUICK_START.md**
   - Training Quick-Start

6. **PI0_CAMERA_CONFIG.md**
   - Deine 4 Camera Konfiguration

7. **PI0_TRAINING_CONFIG_TEMPLATE.md**
   - Config Details & Varianten

8. **PI0_ADAPT_TO_PI_EXPLANATION.md**
   - Warum `adapt_to_pi=False`

9. **PI0_SYNTAX_NOTES.md**
   - Python Syntax Referenz

---

## üîë WICHTIGE PARAMETER & ENTSCHEIDUNGEN

### Training Config (bereits implementiert):

```python
TrainConfig(
    name="pi0_lighter_cup_trossen",
    
    # LoRA Fine-tuning (nicht Full)
    model=pi0.Pi0Config(
        paligemma_variant="gemma_2b_lora",
        action_expert_variant="gemma_300m_lora"
    ),
    
    # Dataset Settings
    data=LeRobotAlohaDataConfig(
        repo_id="MaxFridge/lighter_cup_v2",  # HuggingFace
        use_delta_joint_actions=False,       # Absolute positions
        adapt_to_pi=False,                   # Native Trossen format
        
        # Lokale Assets (nicht Google Cloud)
        assets=AssetsConfig(
            assets_dir=None,
            asset_id=None,
        ),
        
        # Camera Mapping
        repack_transforms=_transforms.Group(
            inputs=[_transforms.RepackTransform({
                "images": {
                    "cam_high": "observation.images.cam_high",
                    "cam_low": "observation.images.cam_low",
                    "cam_left_wrist": "observation.images.left_wrist",
                    "cam_right_wrist": "observation.images.right_wrist",
                },
                "state": "observation.state",
                "actions": "action",
            })]
        ),
    ),
    
    # Training Settings (RTX 6000 Pro optimiert)
    num_train_steps=20_000,
    batch_size=32,  # 4x Standard (dank 96GB VRAM)
    save_interval=5_000,
)
```

### Wichtige Erkenntnisse:

- `adapt_to_pi=False` ‚Üí Native Trossen Format (siehe PI0_ADAPT_TO_PI_EXPLANATION.md)
- `use_delta_joint_actions=False` ‚Üí Absolute Positionen
- `save_interval=5_000` ‚Üí Python Underscores in Zahlen OK (PEP 515)

---

## üíª HARDWARE SETUP

### Training Server (wo du aktuell per SSH eingeloggt bist):

```
Hostname: max-ws (vermutlich)
GPU: NVIDIA RTX 6000 Pro 96GB (Blackwell)
RAM: 64GB+
OS: Ubuntu (vermutlich 22.04)
Location: ~/openpi/
Netzwerk: LAN (IP zu ermitteln mit hostname -I)
```

### Inference PC (lokal, mit Trossen Hardware):

```
GPU: NVIDIA RTX 4080 16GB
RAM: 64GB+ (empfohlen)
OS: Ubuntu (vermutlich 22.04)
Hardware: Trossen AI Stationary Kit
  - 2x WidowX Arms (7 DOF each = 14 total)
  - 4x Cameras (480x640, 30 FPS)
  - USB Verbindungen
  - udev rules konfiguriert (ttyDXL_*, CAM_*)

Bestehende LeRobot Installation:
  - ~/lerobot/ mit ACT Training Setup
  - Conda Environment: lerobot
  - Hardware bereits kalibriert
  - ‚Üí Wiederverwenden f√ºr Pi0!
```

---

## üöÄ N√ÑCHSTE SCHRITTE (Reihenfolge)

### 1. Warte auf Training Completion (~3-4h)

```bash
# Auf Training Server (SSH)
cd ~/openpi

# Monitor Training
tail -f wandb/run-*/logs/debug.log

# Oder WandB Dashboard:
# https://wandb.ai/sourteig-fritsch-gmbh/openpi
```

### 2. Policy Server starten (Training Server)

**Details in:** `docs/PI0_INFERENCE_COMPLETE_GUIDE.md` Abschnitt 4

```bash
cd ~/openpi
uv run scripts/serve_policy.py policy:checkpoint \
  --policy.config=pi0_lighter_cup_trossen \
  --policy.dir=checkpoints/pi0_lighter_cup_trossen/production_v1/20000 \
  --host=0.0.0.0 \
  --port=8000
```

### 3. Client Environment installieren (Inference PC)

**Details in:** `docs/PI0_INFERENCE_COMPLETE_GUIDE.md` Abschnitt 5

```bash
# Auf Inference PC
cd ~/
git clone --recurse-submodules https://github.com/TrossenRobotics/openpi.git openpi_client
cd openpi_client/examples/trossen_ai
GIT_LFS_SKIP_SMUDGE=1 uv sync
```

### 4. Hardware Integration & Testing

**Details in:** `docs/PI0_INFERENCE_COMPLETE_GUIDE.md` Abschnitte 6-8

---

## üìñ F√úR NEUEN CHAT - START HIER

**Kopiere folgende Informationen in neuen Chat:**

```
KONTEXT: Pi0 Inference Setup nach erfolgreichem Training

AKTUELLER STAND:
- Training Server (RTX 6000 Pro 96GB): Training l√§uft/abgeschlossen
- Checkpoint: ~/openpi/checkpoints/pi0_lighter_cup_trossen/production_v1/20000/
- Dataset: MaxFridge/lighter_cup_v2 (92 Episodes, 4 Cameras)
- Configs: 2 Configs erstellt & funktionsf√§hig
- WandB: https://wandb.ai/sourteig-fritsch-gmbh/openpi

AUFGABE: Inference Setup implementieren
- Policy Server auf Training Server
- Robot Client auf Inference PC (RTX 4080)
- Netzwerk-Verbindung
- Hardware-Integration

DOKUMENTATION: ~/lerobot/docs/
- PI0_NEW_CHAT_CONTEXT.md ‚Üê Dieses Dokument
- PI0_INFERENCE_COMPLETE_GUIDE.md ‚Üê Hauptguide

WICHTIG:
- ROS2 NICHT n√∂tig
- Docker NICHT n√∂tig  
- UV Environment (kein Conda)
- Bestehende LeRobot Hardware Config wiederverwenden

HARDWARE:
- Training Server: RTX 6000 Pro 96GB (SSH Remote)
- Inference PC: RTX 4080 16GB (lokal mit Trossen Arms)
- Netzwerk: Gigabit LAN empfohlen

N√ÑCHSTER SCHRITT:
1. Policy Server starten (Training Server)
2. Client Environment installieren (Inference PC)
3. Siehe: docs/PI0_INFERENCE_COMPLETE_GUIDE.md
```

---

## ‚ö° QUICK COMMANDS

### Training Server (Policy Server):

```bash
# Check Training Status
cd ~/openpi
tail -f wandb/run-*/logs/debug.log

# Nach Training ‚Üí Start Policy Server
tmux new -s policy_server
uv run scripts/serve_policy.py policy:checkpoint \
  --policy.config=pi0_lighter_cup_trossen \
  --policy.dir=checkpoints/pi0_lighter_cup_trossen/production_v1/20000 \
  --host=0.0.0.0 \
  --port=8000
```

### Inference PC (Robot Client):

```bash
# Install Client Environment
cd ~/
git clone --recurse-submodules https://github.com/TrossenRobotics/openpi.git openpi_client
cd openpi_client/examples/trossen_ai
GIT_LFS_SKIP_SMUDGE=1 uv sync

# Create Client Script
# (Template in PI0_INFERENCE_COMPLETE_GUIDE.md)

# Run Client
uv run python run_inference_client.py --policy_url ws://<server_ip>:8000
```

---

## üéì LESSONS LEARNED

1. **UV statt Conda:**
   - UV managed Environments in `.venv/` Verzeichnissen
   - Workflow: `cd ~/openpi && uv run ...`
   - KEIN `conda activate` n√∂tig

2. **Zwei LeRobot Versionen:**
   - V0.1.0 f√ºr Training (in openpi)
   - V0.3.2 f√ºr Inference (in openpi_client, BiWidowXAIFollower Support)

3. **AssetsConfig:**
   - `assets_dir=None` ‚Üí lokale Assets in `./assets/`
   - Nicht Google Cloud Storage

4. **adapt_to_pi=False:**
   - Native Trossen Format
   - Keine Joint/Gripper Transformationen
   - Konsistenz Training ‚Üî Inference

---

## üéØ ERFOLGS-METRIKEN

**Training (bereits erreicht):**
- ‚úÖ Config l√§dt ohne Errors
- ‚úÖ Norm Stats erfolgreich berechnet
- ‚úÖ Training startet ohne Errors
- ‚úÖ WandB Logging funktioniert
- ‚úÖ Checkpoints werden gespeichert

**Inference (noch zu erreichen):**
- [ ] Policy Server l√§uft stabil
- [ ] Client verbindet zu Server
- [ ] Hardware wird korrekt angesteuert
- [ ] Latency <50ms (End-to-End)
- [ ] Smoothe, sichere Bewegungen

---

## ‚è≠Ô∏è N√ÑCHSTER CHAT

**Start mit:**
- Lese `docs/PI0_NEW_CHAT_CONTEXT.md` (dieses Dokument)
- Hauptguide: `docs/PI0_INFERENCE_COMPLETE_GUIDE.md`
- Implementiere Policy Server (Training Server)
- Implementiere Robot Client (Inference PC)

**Ziel:**
- Funktionierendes Pi0 Inference System
- Training Server ‚Üí Inference PC √ºber Netzwerk
- Sichere Hardware-Tests
- Performance Benchmarks

**Erwartete Dauer:** 1-2 Tage

**Viel Erfolg! üöÄ**
